apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: default
  labels:
    cni: calico-datastore-k8s
    csi: hcloud-csi
    ccm: hcloud-controller-manager
    metrics-server: metrics-server
spec:
  clusterNetwork:
    pods:
      cidrBlocks: ["10.100.0.0/14"]
    services:
      cidrBlocks: ["192.168.0.0/16"]
    serviceDomain: "cluster.local"
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: "${CLUSTER_NAME}-control-plane"
  infrastructureRef:
    apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
    kind: HcloudCluster
    name: "${CLUSTER_NAME}"
---
apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
kind: HcloudCluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ""
    port: 443
  controlPlaneLoadbalancer:
    type: lb11
    algorithm: round_robin
    services:
    - listenPort: 443
      destinationPort: 6443
      protocol: tcp
  locations:
  - fsn1  # first location is used for location of Loadbalancer
  - nbg1
  - hel1
  network:
    cidrBlock: ""
    subnets: []
  sshKeys:
  - name: cluster
  hcloudTokenRef:
    key: token
    name: hetzner-token
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: "${CLUSTER_NAME}-control-plane"
  namespace: default
spec:
  infrastructureTemplate:
    apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
    kind: HcloudMachineTemplate
    name: "${CLUSTER_NAME}-control-plane-mt-0"
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
      controllerManager:
        extraArgs:
          cloud-provider: external
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,imagefs.inodesFree<20%,memory.available<500Mi
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,imagefs.inodesFree<20%,memory.available<500Mi
  replicas: 3
  version: v1.19.3
---
apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
kind: HcloudMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-control-plane-mt-0"
  namespace: default
spec:
  template:
    spec:
      type: cx21-ceph
      image: centos-8_k8s-v1.19.3
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-worker-kubeadm-0"
  namespace: default
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,imagefs.inodesFree<20%,memory.available<500Mi
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-worker-fsn1"
  namespace: default
  labels:
    nodepool: worker
spec:
  replicas: 1
  clusterName: "${CLUSTER_NAME}"
  selector:
    matchLabels:
      nodepool: worker
  template:
    metadata:
      labels:
        nodepool: worker
    spec:
      clusterName: "${CLUSTER_NAME}"
      version: v1.19.3
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-worker-kubeadm-0"
      infrastructureRef:
        apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
        kind: HcloudMachineTemplate
        name: "${CLUSTER_NAME}-worker-fsn1-mt-0"
      failureDomain: fsn1
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-worker-ngb1"
  namespace: default
  labels:
    nodepool: worker
spec:
  replicas: 1
  clusterName: "${CLUSTER_NAME}"
  selector:
    matchLabels:
      nodepool: worker
  template:
    metadata:
      labels:
        nodepool: worker
    spec:
      clusterName: "${CLUSTER_NAME}"
      version: v1.19.3
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-worker-kubeadm-0"
      infrastructureRef:
        apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
        kind: HcloudMachineTemplate
        name: "${CLUSTER_NAME}-worker-nbg1-mt-0"
      failureDomain: nbg1
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-worker-hel1"
  namespace: default
  labels:
    nodepool: worker
spec:
  replicas: 1
  clusterName: "${CLUSTER_NAME}"
  selector:
    matchLabels:
      nodepool: worker
  template:
    metadata:
      labels:
        nodepool: worker
    spec:
      clusterName: "${CLUSTER_NAME}"
      version: v1.19.3
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-worker-kubeadm-0"
      infrastructureRef:
        apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
        kind: HcloudMachineTemplate
        name: "${CLUSTER_NAME}-worker-hel1-mt-0"
      failureDomain: hel1
---
apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
kind: HcloudMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-worker-fsn1-mt-0"
  namespace: default
spec:
  template:
    spec:
      type: cpx11
      image: centos-8_k8s-v1.19.3
---
apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
kind: HcloudMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-worker-nbg1-mt-0"
  namespace: default
spec:
  template:
    spec:
      type: cpx11
      image: centos-8_k8s-v1.19.3
---
apiVersion: cluster-api-provider-hcloud.capihc.com/v1beta1
kind: HcloudMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-worker-hel1-mt-0"
  namespace: default
spec:
  template:
    spec:
      type: cpx11
      image: centos-8_k8s-v1.19.3
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: "${CLUSTER_NAME}-worker-unhealthy-5m"
spec:
  clusterName: "${CLUSTER_NAME}"
  maxUnhealthy: 10%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      nodepool: worker
  unhealthyConditions:
  - type: Ready
    status: Unknown
    timeout: 300s
  - type: Ready
    status: "False"
    timeout: 300s
